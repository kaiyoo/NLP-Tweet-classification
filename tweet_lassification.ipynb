{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Provided with a collection tweets, the task of this project is to classify whether a tweet constitutes a rumour event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download the tweet corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmy_file = Path(fname)\\nif not my_file.is_file():\\n    url = \"https://github.com/jhlau/jhlau.github.io/blob/master/files/rumour-data.tgz?raw=true\"\\n    r = requests.get(url)\\n\\n    #Save to the current directory\\n    with open(fname, \\'wb\\') as f:\\n        f.write(r.content)\\n        \\nprint(\"Done. File downloaded:\", my_file)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "fname = 'rumour-data.tgz'\n",
    "data_dir = os.path.splitext(fname)[0] #'rumour-data'\n",
    "'''\n",
    "my_file = Path(fname)\n",
    "if not my_file.is_file():\n",
    "    url = \"https://github.com/jhlau/jhlau.github.io/blob/master/files/rumour-data.tgz?raw=true\"\n",
    "    r = requests.get(url)\n",
    "\n",
    "    #Save to the current directory\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        \n",
    "print(\"Done. File downloaded:\", my_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extract the zip file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "#decompress rumour-data.tgz\n",
    "tar = tarfile.open(fname, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "#remove superfluous files (e.g. .DS_store)\n",
    "extra_files = []\n",
    "for r, d, f in os.walk(data_dir):\n",
    "    for file in f:\n",
    "        if (file.startswith(\".\")):\n",
    "            extra_files.append(os.path.join(r, file))\n",
    "for f in extra_files:\n",
    "    os.remove(f)\n",
    "\n",
    "print(\"Extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gather tweet messages from data\n",
    "\n",
    "**Dataset description**: The corpus data is in the *rumour-data* folder. It contains 2 sub-folders: *non-rumours* and *rumours*. *rumours* contains all rumour-propagating tweets, while *non-rumours* has normal tweets. Within  *rumours* and *non-rumours*, there are sub-folders, each named with an ID. Each of these IDs constitutes an 'event', where an event is defined as consisting a **source tweet** and its **reactions**.\n",
    "\n",
    "The folder structure is as follows:\n",
    "\n",
    "    rumour-data\n",
    "        - rumours\n",
    "            - 498254340310966273\n",
    "                - reactions\n",
    "                    - 498254340310966273.json\n",
    "                    - 498260814487642112.json\n",
    "                - source-tweet\n",
    "                    - 498254340310966273.json\n",
    "        - non-rumours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  events completed\n",
      "200  events completed\n",
      "300  events completed\n",
      "400  events completed\n",
      "500  events completed\n",
      "100  events completed\n",
      "200  events completed\n",
      "300  events completed\n",
      "400  events completed\n",
      "500  events completed\n",
      "600  events completed\n",
      "700  events completed\n",
      "800  events completed\n",
      "900  events completed\n",
      "1000  events completed\n",
      "Number of rumour events = 500\n",
      "Number of non-rumour events = 1000\n",
      "104.8161205\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def get_tweet_text_from_json(file_path):\n",
    "    with open(file_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data[\"text\"]\n",
    "    \n",
    "def get_events(event_dir):\n",
    "    event_list = []\n",
    "    for event in sorted(os.listdir(event_dir)):\n",
    "        ###\n",
    "        # Your answer BEGINS HERE\n",
    "        ###\n",
    "        text_list = []\n",
    "        sourch_path = os.path.join(event_dir, event, \"source-tweet\")\n",
    "        reactions_path =  os.path.join(event_dir, event, \"reactions\")\n",
    "    \n",
    "        for jsonfile in os.listdir(sourch_path):\n",
    "            jsonpath = os.path.join(sourch_path, jsonfile) \n",
    "            text_list.append(get_tweet_text_from_json(jsonpath))\n",
    "            \n",
    "        for jsonfile in os.listdir(reactions_path):\n",
    "            jsonpath = os.path.join(reactions_path, jsonfile) \n",
    "            text_list.append(get_tweet_text_from_json(jsonpath))\n",
    "        \n",
    "        event_list.append(text_list)    \n",
    "        listlen = len(event_list)\n",
    "        \n",
    "        if listlen%100 == 0:\n",
    "            print(listlen, \" events completed\")\n",
    "    \n",
    "        ###\n",
    "        # Your answer ENDS HERE\n",
    "        ###\n",
    "        \n",
    "    return event_list\n",
    "\n",
    "start = timer()\n",
    "\n",
    "#a list of events, and each event is a list of tweets (source tweet + reactions)    \n",
    "rumour_events = get_events(os.path.join(data_dir, \"rumours\"))\n",
    "nonrumour_events = get_events(os.path.join(data_dir, \"non-rumours\"))\n",
    "\n",
    "print(\"Number of rumour events =\", len(rumour_events))\n",
    "print(\"Number of non-rumour events =\", len(nonrumour_events))\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(rumour_events) == 500)\n",
    "assert(len(nonrumour_events) == 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing \n",
    "\n",
    "**steps**: (1) tokenize each tweet into individual word tokens; and (2) remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed rumour events = 500\n",
      "Number of preprocessed non-rumour events = 1000\n",
      "1.3023077\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_events(events):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    preprocessed_list = []\n",
    "    for event in events:\n",
    "        tokens = defaultdict(int)\n",
    "        for sent in event:            \n",
    "            tokenized_sentence = tt.tokenize(sent)\n",
    "            for token in tokenized_sentence :\n",
    "                if token.lower() not in stopwords:\n",
    "                    tokens[token] += 1\n",
    "        preprocessed_list.append(tokens)\n",
    "        \n",
    "    return preprocessed_list\n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "start = timer()\n",
    "preprocessed_rumour_events = preprocess_events(rumour_events)\n",
    "preprocessed_nonrumour_events = preprocess_events(nonrumour_events)\n",
    "\n",
    "print(\"Number of preprocessed rumour events =\", len(preprocessed_rumour_events))\n",
    "print(\"Number of preprocessed non-rumour events =\", len(preprocessed_nonrumour_events))\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(preprocessed_rumour_events) == 500)\n",
    "assert(len(preprocessed_nonrumour_events) == 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get hashtags**: Hashtags pose an interesting tokenisation problem because they often include multiple words written without spaces or capitalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hashtags = 1829\n"
     ]
    }
   ],
   "source": [
    "def get_all_hashtags(events):\n",
    "    hashtags = set([])\n",
    "    for event in events:\n",
    "        for word, frequency in event.items():\n",
    "            if word.startswith(\"#\"):\n",
    "                hashtags.add(word)\n",
    "    return hashtags\n",
    "\n",
    "hashtags = get_all_hashtags(preprocessed_rumour_events + preprocessed_nonrumour_events)\n",
    "print(\"Number of hashtags =\", len(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "**MaxMatch**: by a reversed version of the  algorithm, where matching begins at the end of the hashtag and progresses backwards. \n",
    "MaxMatch algorithm should match inflected forms by converting them into lemmas before matching. \n",
    "Part-of-speech tag of the word is provided when lemmatising a word. \n",
    "\n",
    "Example) given \"#speakup\", the algorithm should produce: \\[\"#\", \"speak\", \"up\"\\]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: {'#speakUP': ['#', 'speak', 'UP'], '#speakingup': ['#', 's', 'p', 'e', 'akin', 'gup']}\n",
      "Please wait for 9~15 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words()) #a list of words provided by NLTK\n",
    "vocab = set([word.lower() for word in words]) #lowercased word list\n",
    "\n",
    "# I tried recursive and iterative ways. Recursive way performs slightly faster.\n",
    "'''\n",
    "def max_match(hashtag):\n",
    "    if not hashtag:\n",
    "        return \"\"\n",
    "    parsed_list = [] \n",
    "\n",
    "    for i in range(0, len(hashtag)-1):\n",
    "        first_word = hashtag[i:]\n",
    "        remainder = hashtag[:i]\n",
    "        tagged= nltk.pos_tag([first_word])\n",
    "        tag = word_tag(tagged[0][1])\n",
    "\n",
    "        word = \"\"\n",
    "        if tag is None: \n",
    "            word = first_word.lower()\n",
    "        else:\n",
    "            word = lemmatizer.lemmatize(first_word.lower(), tag)\n",
    "                \n",
    "        if word in vocab:\n",
    "            return first_word + \" \" + max_match(remainder)\n",
    "\n",
    "    first_word = hashtag[-1]\n",
    "    remainder = hashtag[:-1]\n",
    "\n",
    "    return first_word + \" \" + max_match(remainder)    \n",
    "    \n",
    "def tokenize_hashtags(hashtags):  \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    hash_dict = {}\n",
    "    for hashtag in hashtags:\n",
    "        parsed_list = max_match(hashtag).split(\" \")[:-1]\n",
    "        parsed_list.reverse()\n",
    "        hash_dict[hashtag] = parsed_list            \n",
    "    #print(hash_dict)        \n",
    "    return hash_dict           \n",
    "'''\n",
    "\n",
    "def tokenize_hashtags(hashtags):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    hash_dict = {}\n",
    "    for hashtag in hashtags:\n",
    "        len_tag = len(hashtag)   \n",
    "        index = len_tag\n",
    "        parsed_list = []       \n",
    "        while index > 0:\n",
    "            match = False\n",
    "            for i in range(0, index):\n",
    "                word = hashtag[i: index]\n",
    "                tagged= nltk.pos_tag([word])\n",
    "                tag = word_tag(tagged[0][1])\n",
    "                lemma = \"\"\n",
    "                if tag is None: \n",
    "                    lemma = word.lower()\n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word.lower(), tag)                \n",
    "\n",
    "                if lemma in vocab:\n",
    "                    match = True\n",
    "                    parsed_list.append(word)\n",
    "                    index = i\n",
    "                    break\n",
    "            if not match:\n",
    "                parsed_list.append(hashtag[index-1])\n",
    "                index -= 1\n",
    "        parsed_list.reverse()         \n",
    "        hash_dict[hashtag] = parsed_list    \n",
    "        \n",
    "    return hash_dict \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "def word_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None    \n",
    "\n",
    "list_ = ['#speakUP', '#speakingup']\n",
    "print('example:' , tokenize_hashtags(list_))\n",
    "print('Please wait for 9~15 secs')\n",
    "print()\n",
    "start = timer()\n",
    "tokenized_hashtags = tokenize_hashtags(hashtags)\n",
    "print(list(tokenized_hashtags.items())[:20])\n",
    "\n",
    "end = timer()\n",
    "print(end - start)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(tokenized_hashtags) == len(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Update the bag-of-words\n",
    "This function looks for every hashtag in a list of preprocessed events and updates the bag-of-words dictionary with the tokenized hashtag tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed rumour events = 500\n",
      "Number of preprocessed non-rumour events = 1000\n"
     ]
    }
   ],
   "source": [
    "def update_event_bow(events):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    for event in events:\n",
    "        for word, frequency in list(event.items()): #create a snapshot rather than a view to update original one\n",
    "            if word.startswith(\"#\"):\n",
    "                hashtag_parsed = tokenized_hashtags[word]\n",
    "                for item in hashtag_parsed:\n",
    "                    #event[item] += 1\n",
    "                    if item.lower() not in stopwords:         \n",
    "                        event[item] += 1\n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "            \n",
    "update_event_bow(preprocessed_rumour_events)\n",
    "update_event_bow(preprocessed_nonrumour_events)\n",
    "\n",
    "print(\"Number of preprocessed rumour events =\", len(preprocessed_rumour_events))\n",
    "print(\"Number of preprocessed non-rumour events =\", len(preprocessed_nonrumour_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature extraction \n",
    "\n",
    "**data partition**:  create training, development and test partitions with a 60%/20%/20% ratio.\n",
    "\n",
    "**feature extraction**: turn the bag-of-words dictionary of each event into a feature vector, using scikit-learn `DictVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 39516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "events  = preprocessed_rumour_events + preprocessed_nonrumour_events\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(preprocessed_rumour_events)):\n",
    "    y.append(\"rumour\")\n",
    "    \n",
    "for i in range(0, len(preprocessed_nonrumour_events)):\n",
    "    y.append(\"nonrumour\")\n",
    "    \n",
    "X = vectorizer.fit_transform(events)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=94) #62, 94\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=94) \n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###\n",
    "\n",
    "print(\"Vocabulary size =\", len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Hyperparameter tuning\n",
    "\n",
    "**classifiers**: Naive Bayes and Logistic Regression. \n",
    "\n",
    "Hyperparameter tuning using the development set.\n",
    "Cross-validation shouldn't be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= MultinomialNB\n",
      "{'alpha': 0.0}  || accuracy:  0.7633333333333333\n",
      "{'alpha': 0.1}  || accuracy:  0.7433333333333333\n",
      "{'alpha': 0.2}  || accuracy:  0.7466666666666667\n",
      "{'alpha': 0.3}  || accuracy:  0.7466666666666667\n",
      "{'alpha': 0.4}  || accuracy:  0.76\n",
      "{'alpha': 0.5}  || accuracy:  0.7733333333333333\n",
      "{'alpha': 0.6}  || accuracy:  0.7766666666666666\n",
      "{'alpha': 0.7}  || accuracy:  0.7766666666666666\n",
      "{'alpha': 0.8}  || accuracy:  0.7833333333333333\n",
      "{'alpha': 0.9}  || accuracy:  0.7933333333333333\n",
      "{'alpha': 1.0}  || accuracy:  0.8033333333333333\n",
      "best score:  0.8033333333333333  || best param:  {'alpha': 1.0}\n",
      "\n",
      "======= LogisticRegression\n",
      "{'C': 0.001}  || accuracy:  0.7333333333333333\n",
      "{'C': 0.01}  || accuracy:  0.7833333333333333\n",
      "{'C': 0.1}  || accuracy:  0.8233333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DEV\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "D:\\DEV\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}  || accuracy:  0.8133333333333334\n",
      "{'C': 10}  || accuracy:  0.81\n",
      "{'C': 100}  || accuracy:  0.7966666666666666\n",
      "{'C': 1000}  || accuracy:  0.7966666666666666\n",
      "best score:  0.8233333333333334  || best param:  {'C': 0.1}\n",
      "\n",
      "### Best parameters in Development set: \n",
      "Model:  MultinomialNB\n",
      "best parameter: {'alpha': 1.0}\n",
      "best score: 0.8033333333333333\n",
      "\n",
      "Model:  LogisticRegression\n",
      "best parameter: {'C': 0.1}\n",
      "best score: 0.8233333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  accuracy_score, f1_score, classification_report\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "alpha_list = [x/10 for x in range(0, 11)]\n",
    "c_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "nb = [MultinomialNB(alpha=alpha) for alpha in alpha_list]\n",
    "lrcs = [LogisticRegression(C=c) for c in c_list]\n",
    "models = [nb, lrcs]\n",
    "best_params = {}\n",
    "for clfs in models:\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    model_name = clfs[0].__class__.__name__\n",
    "    print()\n",
    "    print(\"=======\", model_name)\n",
    "    for clf in clfs:\n",
    "        model = clf.fit(X_train, y_train)   \n",
    "        predictions = model.predict(X_val)    \n",
    "        accuracy = accuracy_score(y_val ,predictions)\n",
    "        param_dict = None\n",
    "        if model_name == 'MultinomialNB':      \n",
    "            param_dict = {'alpha': clf.get_params()['alpha'] }\n",
    "        else:     \n",
    "            param_dict = {'C': clf.get_params()['C'] }\n",
    "        \n",
    "        print (param_dict, \" || accuracy: \", accuracy)\n",
    "        if accuracy > best_score:\n",
    "            best_score = accuracy\n",
    "            best_param = param_dict\n",
    "            \n",
    "    best_params[model_name] = (best_param , best_score)\n",
    "    print(\"best score: \", best_score, ' || best param: ', best_param)  \n",
    "    \n",
    "print()    \n",
    "print(\"### Best parameters in Development set: \")\n",
    "for model, (param, score) in best_params.items():\n",
    "    print(\"Model: \",  model)\n",
    "    print(\"best parameter:\", param)\n",
    "    print(\"best score:\", score)\n",
    "    print()\n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compare performance\n",
    "Using optimal hyper-parameter settings, compute test performance for Naive Bayes and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB :\n",
      "accuracy:  0.8466666666666667\n",
      "f1_macro:  0.8322310834468003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.91      0.85      0.88       200\n",
      "      rumour       0.74      0.83      0.78       100\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.83      0.84      0.83       300\n",
      "weighted avg       0.85      0.85      0.85       300\n",
      "\n",
      "LogisticRegression :\n",
      "accuracy:  0.8633333333333333\n",
      "f1_macro:  0.8346218487394959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   nonrumour       0.85      0.96      0.90       200\n",
      "      rumour       0.89      0.67      0.77       100\n",
      "\n",
      "    accuracy                           0.86       300\n",
      "   macro avg       0.87      0.81      0.83       300\n",
      "weighted avg       0.87      0.86      0.86       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DEV\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "nb_param, score = best_params['MultinomialNB']\n",
    "log_param, score = best_params['LogisticRegression']\n",
    "models = [     \n",
    "    MultinomialNB(**nb_param),\n",
    "    LogisticRegression(**log_param)\n",
    "]\n",
    "for classifier in models:\n",
    "    model_name = classifier.__class__.__name__\n",
    "    #print(classifier.get_params())\n",
    "    model = classifier.fit(X_train, y_train)   \n",
    "    predictions = model.predict(X_test)   \n",
    "    accuracy = accuracy_score(y_test ,predictions)\n",
    "    f1_macro= f1_score(y_test, predictions, average='macro')\n",
    "    print(model_name , ':')\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"f1_macro: \", f1_macro)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
